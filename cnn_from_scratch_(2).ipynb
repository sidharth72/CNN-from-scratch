{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T03:44:42.077944Z",
          "iopub.execute_input": "2023-06-25T03:44:42.078266Z",
          "iopub.status.idle": "2023-06-25T03:44:53.305942Z",
          "shell.execute_reply.started": "2023-06-25T03:44:42.078240Z",
          "shell.execute_reply": "2023-06-25T03:44:53.304840Z"
        },
        "trusted": true,
        "id": "iA19hzmZqUym"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the orientation of the dataset inorder to work"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-18T05:44:00.657105Z",
          "iopub.execute_input": "2023-06-18T05:44:00.657462Z",
          "iopub.status.idle": "2023-06-18T05:44:00.663447Z",
          "shell.execute_reply.started": "2023-06-18T05:44:00.657433Z",
          "shell.execute_reply": "2023-06-18T05:44:00.662463Z"
        },
        "trusted": true,
        "id": "W23sr08AqUy5"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import correlate2d, convolve2d\n",
        "\n",
        "class Convolution:\n",
        "\n",
        "    def __init__(self, input_shape, filter_size, num_filters):\n",
        "        input_height, input_width = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        # Size of outputs and filters or kernels or weigths\n",
        "\n",
        "        self.filter_shape = (num_filters, filter_size, filter_size) # (3,3)\n",
        "        self.output_shape = (num_filters, input_height - filter_size + 1, input_width - filter_size + 1)\n",
        "\n",
        "        self.filters = np.random.randn(*self.filter_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "\n",
        "    def cross_correlation2D(self, input_mat, filter_mat):\n",
        "        # Get the dimensions of the image and kernel\n",
        "        image_height, image_width = input_mat.shape\n",
        "        # (3, 3) shape\n",
        "        kernel_height, kernel_width = filter_mat.shape\n",
        "\n",
        "        # Calculate the output dimensions\n",
        "        output_height = image_height - kernel_height + 1\n",
        "        output_width = image_width - kernel_width + 1\n",
        "\n",
        "        # Create an empty array to store the cross-correlation result\n",
        "        result = np.zeros((output_height, output_width))\n",
        "\n",
        "        # Perform cross-correlation\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                patch = input_mat[i:i + kernel_height, j:j + kernel_width]\n",
        "                result[i, j] = np.sum(patch * filter_mat)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        # Initialized the input value\n",
        "        output = np.zeros(self.output_shape)\n",
        "        for i in range(self.num_filters):\n",
        "            output[i] = correlate2d(self.input_data, self.filters[i], mode=\"valid\")\n",
        "        #Applying Relu Activtion function\n",
        "        output = np.maximum(output, 0)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def backward(self, dL_dout, lr):\n",
        "        # Create a random dL_dout array\n",
        "        dL_dinput = np.zeros_like(self.input_data)\n",
        "        dL_dfilters = np.zeros_like(self.filters)\n",
        "\n",
        "        for i in range(self.num_filters):\n",
        "                # Calculating the gradient of the filters or weights\n",
        "                dL_dfilters[i] = correlate2d(self.input_data, dL_dout[i],mode=\"valid\")\n",
        "\n",
        "                # Calculating the gradient of the inputs\n",
        "                dL_dinput += convolve2d(dL_dout[i], self.filters[i], mode=\"full\")\n",
        "\n",
        "        self.filters -= lr * dL_dfilters\n",
        "        self.biases -= lr * dL_dout\n",
        "\n",
        "\n",
        "        return dL_dinput\n",
        "\n",
        "\n",
        "class MaxPool:\n",
        "\n",
        "    def __init__(self, pool_size):\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.num_channels, self.input_height, self.input_width = input_data.shape\n",
        "        self.output_height = self.input_height // self.pool_size\n",
        "        self.output_width = self.input_width // self.pool_size\n",
        "\n",
        "        self.output = np.zeros((self.num_channels, self.output_height, self.output_width))\n",
        "\n",
        "\n",
        "        for c in range(self.num_channels):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "                    patch = input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "                    self.output[c, i, j] = np.max(patch)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dL_dout, lr):\n",
        "        dL_dinput = np.zeros_like(self.input_data)\n",
        "\n",
        "        for c in range(self.num_channels):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "                    patch = self.input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "                    mask = patch == np.max(patch)\n",
        "\n",
        "                    dL_dinput[c,start_i:end_i, start_j:end_j] = dL_dout[c, i, j] * mask\n",
        "\n",
        "        return dL_dinput\n",
        "\n",
        "class Fully_Connected:\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        #self.ouptut_neurons = output_neurons\n",
        "        self.weights = np.random.randn(output_size, self.input_size)\n",
        "        #print(self.weights)\n",
        "        self.biases = np.random.rand(output_size, 1)\n",
        "\n",
        "\n",
        "    def softmax(self, z):\n",
        "        # Shift the input values to avoid numerical instability\n",
        "        shifted_z = z - np.max(z)\n",
        "        # Compute the exponentiated values\n",
        "        exp_values = np.exp(shifted_z)\n",
        "\n",
        "        # Compute the sum of exponentiated values using log-sum-exp trick\n",
        "        sum_exp_values = np.sum(exp_values, axis=0)\n",
        "        log_sum_exp = np.log(sum_exp_values)\n",
        "\n",
        "        # Compute the softmax probabilities\n",
        "        probabilities = exp_values / sum_exp_values\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "\n",
        "    def softmax_derivative(self, s):\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        flattened_input = input_data.flatten().reshape(1, -1)\n",
        "        self.z = np.dot(self.weights, flattened_input.T) + self.biases\n",
        "        self.output = self.softmax(self.z)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dL_dout, lr):\n",
        "        # Calculate the gradient of the loss with respect to the pre-activation (z)\n",
        "        dL_dy = np.dot(self.softmax_derivative(self.output), dL_dout)\n",
        "        # Calculate the gradient of the loss with respect to the weights (dw)\n",
        "        dL_dw = np.dot(dL_dy, self.input_data.flatten().reshape(1, -1))\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to the biases (db)\n",
        "        dL_db = dL_dy\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to the input data (dL_dinput)\n",
        "        dL_dinput = np.dot(self.weights.T, dL_dy)\n",
        "        dL_dinput = dL_dinput.reshape(self.input_data.shape)\n",
        "\n",
        "        # Update the weights and biases based on the learning rate and gradients\n",
        "        self.weights -= lr * dL_dw\n",
        "        self.biases -= lr * dL_db\n",
        "\n",
        "        # Return the gradient of the loss with respect to the input data\n",
        "        return dL_dinput"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T03:45:40.205976Z",
          "iopub.execute_input": "2023-06-25T03:45:40.206714Z",
          "iopub.status.idle": "2023-06-25T03:45:40.723064Z",
          "shell.execute_reply.started": "2023-06-25T03:45:40.206680Z",
          "shell.execute_reply": "2023-06-25T03:45:40.722123Z"
        },
        "trusted": true,
        "id": "PvIMV8ZmqUy8"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_images[:3000] / 255.0\n",
        "y_train = train_labels[:3000]\n",
        "\n",
        "X_test = train_images[3000:60000] / 255.0\n",
        "y_test = train_labels[3000:60000]\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T03:45:41.453477Z",
          "iopub.execute_input": "2023-06-25T03:45:41.453850Z",
          "iopub.status.idle": "2023-06-25T03:45:41.473422Z",
          "shell.execute_reply.started": "2023-06-25T03:45:41.453816Z",
          "shell.execute_reply": "2023-06-25T03:45:41.472249Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBfPAwoNqUzZ",
        "outputId": "2498b60c-b83a-48b6-97ed-54db5099accf"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_test[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T03:45:45.198492Z",
          "iopub.execute_input": "2023-06-25T03:45:45.198853Z",
          "iopub.status.idle": "2023-06-25T03:45:45.208030Z",
          "shell.execute_reply.started": "2023-06-25T03:45:45.198818Z",
          "shell.execute_reply": "2023-06-25T03:45:45.206722Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIPKkPTIqUzb",
        "outputId": "693dd181-6c69-4dcb-85db-62f2f39e1a0e"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Convolution(X_train[0].shape, 6, 1)\n",
        "pool = MaxPool(2)\n",
        "full = Fully_Connected(121,10)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(predictions, targets):\n",
        "\n",
        "    num_samples = 10\n",
        "\n",
        "    # Avoid numerical instability by adding a small epsilon value\n",
        "    epsilon = 1e-7\n",
        "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Calculate the categorical cross-entropy loss\n",
        "    loss = -np.sum(targets * np.log(predictions)) / num_samples\n",
        "\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
        "    num_samples = actual_labels.shape[0]\n",
        "\n",
        "    # Calculate the gradient of the cross-entropy loss function\n",
        "    gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples\n",
        "\n",
        "    return gradient\n",
        "\n",
        "\n",
        "def predict(input_sample, conv, pool, full):\n",
        "    # Forward pass through convolutional and pooling layers\n",
        "    conv_out = conv.forward(input_sample)\n",
        "    pool_out = pool.forward(conv_out)\n",
        "\n",
        "    # Flatten the output feature maps\n",
        "    flattened_output = pool_out.flatten()\n",
        "\n",
        "    # Forward pass through fully connected layer\n",
        "    predictions = full.forward(flattened_output)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def train_network(X, y, conv, pool, full, lr=0.01, epochs=200):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            # Forward pass\n",
        "            conv_out = conv.forward(X[i])\n",
        "            pool_out = pool.forward(conv_out)\n",
        "            full_out = full.forward(pool_out)\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            loss = cross_entropy_loss(full_out.flatten(), y[i])\n",
        "            total_loss += loss\n",
        "\n",
        "\n",
        "            one_hot_pred = np.zeros_like(full_out)\n",
        "            one_hot_pred[np.argmax(full_out)] = 1\n",
        "            one_hot_pred = one_hot_pred.flatten()\n",
        "\n",
        "            num_pred = np.argmax(one_hot_pred)\n",
        "            num_y = np.argmax(y[i])\n",
        "\n",
        "            if num_pred == num_y:\n",
        "                correct_predictions += 1\n",
        "            # Backward pass\n",
        "            gradient = cross_entropy_loss_gradient(y[i], full_out.flatten()).reshape((-1, 1))\n",
        "            full_back = full.backward(gradient, lr)\n",
        "            pool_back = pool.backward(full_back, lr)\n",
        "            conv_back = conv.backward(pool_back, lr)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        average_loss = total_loss / len(X)\n",
        "        accuracy = correct_predictions / len(X_train) * 100.0\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T03:45:47.890466Z",
          "iopub.execute_input": "2023-06-25T03:45:47.890824Z",
          "iopub.status.idle": "2023-06-25T03:45:47.906346Z",
          "shell.execute_reply.started": "2023-06-25T03:45:47.890794Z",
          "shell.execute_reply": "2023-06-25T03:45:47.905219Z"
        },
        "trusted": true,
        "id": "Ay4RrvE_qUzd"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_network(X_train, y_train, conv, pool, full)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-25T03:45:53.408334Z",
          "iopub.execute_input": "2023-06-25T03:45:53.408689Z",
          "iopub.status.idle": "2023-06-25T03:46:05.158060Z",
          "shell.execute_reply.started": "2023-06-25T03:45:53.408662Z",
          "shell.execute_reply": "2023-06-25T03:46:05.155586Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCM08pASqUzj",
        "outputId": "e96dca1e-ff2b-4edc-c204-2f810670acf6"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 - Loss: 0.9825 - Accuracy: 18.37%\n",
            "Epoch 2/200 - Loss: 0.5256 - Accuracy: 22.83%\n",
            "Epoch 3/200 - Loss: 0.2406 - Accuracy: 29.53%\n",
            "Epoch 4/200 - Loss: 0.1882 - Accuracy: 37.20%\n",
            "Epoch 5/200 - Loss: 0.1693 - Accuracy: 42.37%\n",
            "Epoch 6/200 - Loss: 0.1577 - Accuracy: 45.40%\n",
            "Epoch 7/200 - Loss: 0.1485 - Accuracy: 48.83%\n",
            "Epoch 8/200 - Loss: 0.1412 - Accuracy: 51.10%\n",
            "Epoch 9/200 - Loss: 0.1349 - Accuracy: 53.50%\n",
            "Epoch 10/200 - Loss: 0.1297 - Accuracy: 54.83%\n",
            "Epoch 11/200 - Loss: 0.1254 - Accuracy: 56.90%\n",
            "Epoch 12/200 - Loss: 0.1215 - Accuracy: 57.63%\n",
            "Epoch 13/200 - Loss: 0.1181 - Accuracy: 59.00%\n",
            "Epoch 14/200 - Loss: 0.1150 - Accuracy: 59.63%\n",
            "Epoch 15/200 - Loss: 0.1124 - Accuracy: 60.37%\n",
            "Epoch 16/200 - Loss: 0.1097 - Accuracy: 61.60%\n",
            "Epoch 17/200 - Loss: 0.1076 - Accuracy: 61.87%\n",
            "Epoch 18/200 - Loss: 0.1056 - Accuracy: 62.53%\n",
            "Epoch 19/200 - Loss: 0.1038 - Accuracy: 63.47%\n",
            "Epoch 20/200 - Loss: 0.1021 - Accuracy: 64.13%\n",
            "Epoch 21/200 - Loss: 0.1004 - Accuracy: 64.57%\n",
            "Epoch 22/200 - Loss: 0.0989 - Accuracy: 65.63%\n",
            "Epoch 23/200 - Loss: 0.0976 - Accuracy: 65.77%\n",
            "Epoch 24/200 - Loss: 0.0963 - Accuracy: 65.97%\n",
            "Epoch 25/200 - Loss: 0.0951 - Accuracy: 66.43%\n",
            "Epoch 26/200 - Loss: 0.0939 - Accuracy: 67.37%\n",
            "Epoch 27/200 - Loss: 0.0929 - Accuracy: 67.83%\n",
            "Epoch 28/200 - Loss: 0.0919 - Accuracy: 67.90%\n",
            "Epoch 29/200 - Loss: 0.0908 - Accuracy: 68.30%\n",
            "Epoch 30/200 - Loss: 0.0900 - Accuracy: 68.80%\n",
            "Epoch 31/200 - Loss: 0.0891 - Accuracy: 68.97%\n",
            "Epoch 32/200 - Loss: 0.0883 - Accuracy: 69.07%\n",
            "Epoch 33/200 - Loss: 0.0876 - Accuracy: 69.20%\n",
            "Epoch 34/200 - Loss: 0.0866 - Accuracy: 69.70%\n",
            "Epoch 35/200 - Loss: 0.0859 - Accuracy: 70.10%\n",
            "Epoch 36/200 - Loss: 0.0851 - Accuracy: 70.33%\n",
            "Epoch 37/200 - Loss: 0.0844 - Accuracy: 70.60%\n",
            "Epoch 38/200 - Loss: 0.0838 - Accuracy: 70.70%\n",
            "Epoch 39/200 - Loss: 0.0832 - Accuracy: 70.93%\n",
            "Epoch 40/200 - Loss: 0.0825 - Accuracy: 71.23%\n",
            "Epoch 41/200 - Loss: 0.0819 - Accuracy: 71.63%\n",
            "Epoch 42/200 - Loss: 0.0815 - Accuracy: 71.67%\n",
            "Epoch 43/200 - Loss: 0.0809 - Accuracy: 72.10%\n",
            "Epoch 44/200 - Loss: 0.0804 - Accuracy: 72.13%\n",
            "Epoch 45/200 - Loss: 0.0798 - Accuracy: 72.17%\n",
            "Epoch 46/200 - Loss: 0.0794 - Accuracy: 72.37%\n",
            "Epoch 47/200 - Loss: 0.0789 - Accuracy: 72.40%\n",
            "Epoch 48/200 - Loss: 0.0785 - Accuracy: 72.83%\n",
            "Epoch 49/200 - Loss: 0.0781 - Accuracy: 72.97%\n",
            "Epoch 50/200 - Loss: 0.0776 - Accuracy: 72.77%\n",
            "Epoch 51/200 - Loss: 0.0772 - Accuracy: 73.57%\n",
            "Epoch 52/200 - Loss: 0.0768 - Accuracy: 73.40%\n",
            "Epoch 53/200 - Loss: 0.0765 - Accuracy: 73.57%\n",
            "Epoch 54/200 - Loss: 0.0759 - Accuracy: 73.93%\n",
            "Epoch 55/200 - Loss: 0.0756 - Accuracy: 73.87%\n",
            "Epoch 56/200 - Loss: 0.0753 - Accuracy: 74.23%\n",
            "Epoch 57/200 - Loss: 0.0749 - Accuracy: 74.13%\n",
            "Epoch 58/200 - Loss: 0.0746 - Accuracy: 74.27%\n",
            "Epoch 59/200 - Loss: 0.0743 - Accuracy: 74.63%\n",
            "Epoch 60/200 - Loss: 0.0740 - Accuracy: 74.53%\n",
            "Epoch 61/200 - Loss: 0.0737 - Accuracy: 74.43%\n",
            "Epoch 62/200 - Loss: 0.0735 - Accuracy: 74.57%\n",
            "Epoch 63/200 - Loss: 0.0731 - Accuracy: 74.70%\n",
            "Epoch 64/200 - Loss: 0.0728 - Accuracy: 74.77%\n",
            "Epoch 65/200 - Loss: 0.0726 - Accuracy: 75.00%\n",
            "Epoch 66/200 - Loss: 0.0723 - Accuracy: 74.97%\n",
            "Epoch 67/200 - Loss: 0.0720 - Accuracy: 75.17%\n",
            "Epoch 68/200 - Loss: 0.0717 - Accuracy: 75.20%\n",
            "Epoch 69/200 - Loss: 0.0715 - Accuracy: 75.50%\n",
            "Epoch 70/200 - Loss: 0.0712 - Accuracy: 75.37%\n",
            "Epoch 71/200 - Loss: 0.0709 - Accuracy: 75.73%\n",
            "Epoch 72/200 - Loss: 0.0708 - Accuracy: 75.63%\n",
            "Epoch 73/200 - Loss: 0.0705 - Accuracy: 75.90%\n",
            "Epoch 74/200 - Loss: 0.0702 - Accuracy: 75.63%\n",
            "Epoch 75/200 - Loss: 0.0701 - Accuracy: 75.67%\n",
            "Epoch 76/200 - Loss: 0.0698 - Accuracy: 75.80%\n",
            "Epoch 77/200 - Loss: 0.0696 - Accuracy: 75.67%\n",
            "Epoch 78/200 - Loss: 0.0694 - Accuracy: 76.03%\n",
            "Epoch 79/200 - Loss: 0.0692 - Accuracy: 76.13%\n",
            "Epoch 80/200 - Loss: 0.0689 - Accuracy: 76.13%\n",
            "Epoch 81/200 - Loss: 0.0688 - Accuracy: 76.03%\n",
            "Epoch 82/200 - Loss: 0.0687 - Accuracy: 76.13%\n",
            "Epoch 83/200 - Loss: 0.0684 - Accuracy: 76.20%\n",
            "Epoch 84/200 - Loss: 0.0682 - Accuracy: 76.33%\n",
            "Epoch 85/200 - Loss: 0.0681 - Accuracy: 76.10%\n",
            "Epoch 86/200 - Loss: 0.0678 - Accuracy: 76.13%\n",
            "Epoch 87/200 - Loss: 0.0676 - Accuracy: 76.27%\n",
            "Epoch 88/200 - Loss: 0.0675 - Accuracy: 76.40%\n",
            "Epoch 89/200 - Loss: 0.0673 - Accuracy: 76.30%\n",
            "Epoch 90/200 - Loss: 0.0671 - Accuracy: 76.27%\n",
            "Epoch 91/200 - Loss: 0.0669 - Accuracy: 76.37%\n",
            "Epoch 92/200 - Loss: 0.0669 - Accuracy: 76.47%\n",
            "Epoch 93/200 - Loss: 0.0666 - Accuracy: 76.40%\n",
            "Epoch 94/200 - Loss: 0.0664 - Accuracy: 76.47%\n",
            "Epoch 95/200 - Loss: 0.0665 - Accuracy: 76.60%\n",
            "Epoch 96/200 - Loss: 0.0662 - Accuracy: 76.80%\n",
            "Epoch 97/200 - Loss: 0.0659 - Accuracy: 76.83%\n",
            "Epoch 98/200 - Loss: 0.0659 - Accuracy: 76.77%\n",
            "Epoch 99/200 - Loss: 0.0657 - Accuracy: 76.90%\n",
            "Epoch 100/200 - Loss: 0.0655 - Accuracy: 77.00%\n",
            "Epoch 101/200 - Loss: 0.0653 - Accuracy: 77.13%\n",
            "Epoch 102/200 - Loss: 0.0653 - Accuracy: 77.10%\n",
            "Epoch 103/200 - Loss: 0.0652 - Accuracy: 77.27%\n",
            "Epoch 104/200 - Loss: 0.0651 - Accuracy: 77.10%\n",
            "Epoch 105/200 - Loss: 0.0649 - Accuracy: 77.13%\n",
            "Epoch 106/200 - Loss: 0.0648 - Accuracy: 77.13%\n",
            "Epoch 107/200 - Loss: 0.0647 - Accuracy: 77.17%\n",
            "Epoch 108/200 - Loss: 0.0645 - Accuracy: 77.43%\n",
            "Epoch 109/200 - Loss: 0.0643 - Accuracy: 77.40%\n",
            "Epoch 110/200 - Loss: 0.0642 - Accuracy: 77.37%\n",
            "Epoch 111/200 - Loss: 0.0643 - Accuracy: 77.33%\n",
            "Epoch 112/200 - Loss: 0.0640 - Accuracy: 77.60%\n",
            "Epoch 113/200 - Loss: 0.0639 - Accuracy: 77.63%\n",
            "Epoch 114/200 - Loss: 0.0637 - Accuracy: 77.50%\n",
            "Epoch 115/200 - Loss: 0.0636 - Accuracy: 77.63%\n",
            "Epoch 116/200 - Loss: 0.0635 - Accuracy: 77.67%\n",
            "Epoch 117/200 - Loss: 0.0635 - Accuracy: 77.60%\n",
            "Epoch 118/200 - Loss: 0.0633 - Accuracy: 77.63%\n",
            "Epoch 119/200 - Loss: 0.0631 - Accuracy: 77.67%\n",
            "Epoch 120/200 - Loss: 0.0630 - Accuracy: 77.67%\n",
            "Epoch 121/200 - Loss: 0.0629 - Accuracy: 77.83%\n",
            "Epoch 122/200 - Loss: 0.0628 - Accuracy: 77.80%\n",
            "Epoch 123/200 - Loss: 0.0628 - Accuracy: 78.00%\n",
            "Epoch 124/200 - Loss: 0.0626 - Accuracy: 78.00%\n",
            "Epoch 125/200 - Loss: 0.0625 - Accuracy: 77.90%\n",
            "Epoch 126/200 - Loss: 0.0624 - Accuracy: 78.10%\n",
            "Epoch 127/200 - Loss: 0.0623 - Accuracy: 78.20%\n",
            "Epoch 128/200 - Loss: 0.0621 - Accuracy: 78.13%\n",
            "Epoch 129/200 - Loss: 0.0620 - Accuracy: 78.13%\n",
            "Epoch 130/200 - Loss: 0.0619 - Accuracy: 78.40%\n",
            "Epoch 131/200 - Loss: 0.0618 - Accuracy: 78.23%\n",
            "Epoch 132/200 - Loss: 0.0618 - Accuracy: 78.43%\n",
            "Epoch 133/200 - Loss: 0.0616 - Accuracy: 78.40%\n",
            "Epoch 134/200 - Loss: 0.0614 - Accuracy: 78.40%\n",
            "Epoch 135/200 - Loss: 0.0614 - Accuracy: 78.43%\n",
            "Epoch 136/200 - Loss: 0.0613 - Accuracy: 78.47%\n",
            "Epoch 137/200 - Loss: 0.0611 - Accuracy: 78.57%\n",
            "Epoch 138/200 - Loss: 0.0611 - Accuracy: 78.60%\n",
            "Epoch 139/200 - Loss: 0.0610 - Accuracy: 78.77%\n",
            "Epoch 140/200 - Loss: 0.0609 - Accuracy: 78.70%\n",
            "Epoch 141/200 - Loss: 0.0608 - Accuracy: 78.77%\n",
            "Epoch 142/200 - Loss: 0.0607 - Accuracy: 78.97%\n",
            "Epoch 143/200 - Loss: 0.0606 - Accuracy: 78.90%\n",
            "Epoch 144/200 - Loss: 0.0605 - Accuracy: 78.97%\n",
            "Epoch 145/200 - Loss: 0.0604 - Accuracy: 78.90%\n",
            "Epoch 146/200 - Loss: 0.0603 - Accuracy: 79.07%\n",
            "Epoch 147/200 - Loss: 0.0602 - Accuracy: 79.00%\n",
            "Epoch 148/200 - Loss: 0.0601 - Accuracy: 79.13%\n",
            "Epoch 149/200 - Loss: 0.0600 - Accuracy: 79.10%\n",
            "Epoch 150/200 - Loss: 0.0599 - Accuracy: 79.17%\n",
            "Epoch 151/200 - Loss: 0.0598 - Accuracy: 79.30%\n",
            "Epoch 152/200 - Loss: 0.0597 - Accuracy: 79.23%\n",
            "Epoch 153/200 - Loss: 0.0597 - Accuracy: 79.33%\n",
            "Epoch 154/200 - Loss: 0.0596 - Accuracy: 79.20%\n",
            "Epoch 155/200 - Loss: 0.0595 - Accuracy: 79.30%\n",
            "Epoch 156/200 - Loss: 0.0594 - Accuracy: 79.10%\n",
            "Epoch 157/200 - Loss: 0.0592 - Accuracy: 79.30%\n",
            "Epoch 158/200 - Loss: 0.0592 - Accuracy: 79.47%\n",
            "Epoch 159/200 - Loss: 0.0591 - Accuracy: 79.47%\n",
            "Epoch 160/200 - Loss: 0.0591 - Accuracy: 79.47%\n",
            "Epoch 161/200 - Loss: 0.0589 - Accuracy: 79.63%\n",
            "Epoch 162/200 - Loss: 0.0589 - Accuracy: 79.47%\n",
            "Epoch 163/200 - Loss: 0.0587 - Accuracy: 79.47%\n",
            "Epoch 164/200 - Loss: 0.0586 - Accuracy: 79.60%\n",
            "Epoch 165/200 - Loss: 0.0586 - Accuracy: 79.60%\n",
            "Epoch 166/200 - Loss: 0.0585 - Accuracy: 79.50%\n",
            "Epoch 167/200 - Loss: 0.0584 - Accuracy: 79.60%\n",
            "Epoch 168/200 - Loss: 0.0584 - Accuracy: 79.50%\n",
            "Epoch 169/200 - Loss: 0.0583 - Accuracy: 79.77%\n",
            "Epoch 170/200 - Loss: 0.0582 - Accuracy: 79.73%\n",
            "Epoch 171/200 - Loss: 0.0581 - Accuracy: 79.73%\n",
            "Epoch 172/200 - Loss: 0.0580 - Accuracy: 79.87%\n",
            "Epoch 173/200 - Loss: 0.0579 - Accuracy: 80.00%\n",
            "Epoch 174/200 - Loss: 0.0578 - Accuracy: 79.77%\n",
            "Epoch 175/200 - Loss: 0.0577 - Accuracy: 79.90%\n",
            "Epoch 176/200 - Loss: 0.0576 - Accuracy: 79.90%\n",
            "Epoch 177/200 - Loss: 0.0575 - Accuracy: 79.97%\n",
            "Epoch 178/200 - Loss: 0.0575 - Accuracy: 79.90%\n",
            "Epoch 179/200 - Loss: 0.0574 - Accuracy: 79.97%\n",
            "Epoch 180/200 - Loss: 0.0574 - Accuracy: 79.80%\n",
            "Epoch 181/200 - Loss: 0.0573 - Accuracy: 79.93%\n",
            "Epoch 182/200 - Loss: 0.0572 - Accuracy: 79.93%\n",
            "Epoch 183/200 - Loss: 0.0571 - Accuracy: 80.00%\n",
            "Epoch 184/200 - Loss: 0.0571 - Accuracy: 79.87%\n",
            "Epoch 185/200 - Loss: 0.0570 - Accuracy: 79.83%\n",
            "Epoch 186/200 - Loss: 0.0569 - Accuracy: 79.83%\n",
            "Epoch 187/200 - Loss: 0.0568 - Accuracy: 80.17%\n",
            "Epoch 188/200 - Loss: 0.0567 - Accuracy: 80.07%\n",
            "Epoch 189/200 - Loss: 0.0567 - Accuracy: 80.13%\n",
            "Epoch 190/200 - Loss: 0.0567 - Accuracy: 80.17%\n",
            "Epoch 191/200 - Loss: 0.0566 - Accuracy: 80.03%\n",
            "Epoch 192/200 - Loss: 0.0565 - Accuracy: 80.20%\n",
            "Epoch 193/200 - Loss: 0.0564 - Accuracy: 80.27%\n",
            "Epoch 194/200 - Loss: 0.0564 - Accuracy: 80.30%\n",
            "Epoch 195/200 - Loss: 0.0563 - Accuracy: 80.30%\n",
            "Epoch 196/200 - Loss: 0.0562 - Accuracy: 80.30%\n",
            "Epoch 197/200 - Loss: 0.0562 - Accuracy: 80.27%\n",
            "Epoch 198/200 - Loss: 0.0561 - Accuracy: 80.33%\n",
            "Epoch 199/200 - Loss: 0.0560 - Accuracy: 80.27%\n",
            "Epoch 200/200 - Loss: 0.0559 - Accuracy: 80.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2zLACdtqUzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for data in X_test:\n",
        "    pred = predict(data, conv, pool, full)\n",
        "    one_hot_pred = np.zeros_like(pred)\n",
        "    one_hot_pred[np.argmax(pred)] = 1\n",
        "    predictions.append(one_hot_pred.flatten())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-18T05:48:04.705858Z",
          "iopub.execute_input": "2023-06-18T05:48:04.706208Z",
          "iopub.status.idle": "2023-06-18T05:48:04.812298Z",
          "shell.execute_reply.started": "2023-06-18T05:48:04.706180Z",
          "shell.execute_reply": "2023-06-18T05:48:04.810838Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhGrjqfWqUzm",
        "outputId": "86005b91-a98b-4ef4-b9e8-495173235e8e"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(predictions, y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-18T05:48:06.563460Z",
          "iopub.execute_input": "2023-06-18T05:48:06.563848Z",
          "iopub.status.idle": "2023-06-18T05:48:07.057561Z",
          "shell.execute_reply.started": "2023-06-18T05:48:06.563820Z",
          "shell.execute_reply": "2023-06-18T05:48:07.055737Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2e3Q4JaqUzp",
        "outputId": "76149046-570b-4b26-922f-ccf492b719a1"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7672456140350877"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-14T13:22:29.427915Z",
          "iopub.execute_input": "2023-06-14T13:22:29.428280Z",
          "iopub.status.idle": "2023-06-14T13:22:29.436009Z",
          "shell.execute_reply.started": "2023-06-14T13:22:29.428248Z",
          "shell.execute_reply": "2023-06-14T13:22:29.434933Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Lf7JD8qUzt",
        "outputId": "ede987b8-2893-4240-ff6e-1f92f9d80a85"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-14T11:23:38.026846Z",
          "iopub.execute_input": "2023-06-14T11:23:38.027546Z",
          "iopub.status.idle": "2023-06-14T11:23:38.034962Z",
          "shell.execute_reply.started": "2023-06-14T11:23:38.027511Z",
          "shell.execute_reply": "2023-06-14T11:23:38.033828Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvMk1evmqUzw",
        "outputId": "7e75ae62-3228-43e3-8bf7-a7617172747e"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(predictions, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhSR2SInqUz3",
        "outputId": "6352052e-0802-4682-9b1b-a1a1c22414a8"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.79      0.74      5093\n",
            "           1       0.91      0.92      0.91      5645\n",
            "           2       0.52      0.70      0.59      4244\n",
            "           3       0.81      0.79      0.80      5904\n",
            "           4       0.68      0.66      0.67      5891\n",
            "           5       0.83      0.87      0.85      5473\n",
            "           6       0.56      0.43      0.49      7405\n",
            "           7       0.86      0.84      0.85      5765\n",
            "           8       0.90      0.89      0.89      5764\n",
            "           9       0.91      0.89      0.90      5816\n",
            "\n",
            "   micro avg       0.77      0.77      0.77     57000\n",
            "   macro avg       0.77      0.78      0.77     57000\n",
            "weighted avg       0.77      0.77      0.77     57000\n",
            " samples avg       0.77      0.77      0.77     57000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nn7MVUIsAzR-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}